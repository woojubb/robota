/**
 * 06-token-and-request-limits-simple.ts
 * 
 * í† í° ë° ìš”ì²­ ì œí•œ ê¸°ëŠ¥ì˜ ê°„ë‹¨í•œ ì‚¬ìš©ë²•:
 * - ê¸°ë³¸ ì„¤ì •ê³¼ ì»¤ìŠ¤í…€ ì„¤ì •
 * - ì œí•œ í™•ì¸ ë° ëª¨ë‹ˆí„°ë§
 * - ì—ëŸ¬ ì²˜ë¦¬
 */

import { Robota, OpenAIProvider } from '@robota-sdk/core';
import OpenAI from 'openai';
import dotenv from 'dotenv';

dotenv.config();

async function main() {
    const apiKey = process.env.OPENAI_API_KEY;
    if (!apiKey) {
        throw new Error('OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤');
    }

    const openaiClient = new OpenAI({ apiKey });
    const openaiProvider = new OpenAIProvider(openaiClient);

    console.log('ðŸ”§ ê°„ë‹¨í•œ í† í°/ìš”ì²­ ì œí•œ ì˜ˆì œ\n');

    // 1. ê¸°ë³¸ ì„¤ì • (maxTokens: 4096, maxRequests: 25)
    console.log('=== ê¸°ë³¸ ì„¤ì • ===');
    const robota1 = new Robota({
        aiProviders: { 'openai': openaiProvider },
        currentProvider: 'openai',
        currentModel: 'gpt-3.5-turbo'
    });

    console.log(`ê¸°ë³¸ í† í° ì œí•œ: ${robota1.getMaxTokenLimit()}`);
    console.log(`ê¸°ë³¸ ìš”ì²­ ì œí•œ: ${robota1.getMaxRequestLimit()}`);

    // 2. ì»¤ìŠ¤í…€ ì„¤ì •
    console.log('\n=== ì»¤ìŠ¤í…€ ì„¤ì • ===');
    const robota2 = new Robota({
        aiProviders: { 'openai': openaiProvider },
        currentProvider: 'openai',
        currentModel: 'gpt-3.5-turbo',
        systemPrompt: 'ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.',
        maxTokenLimit: 100,  // ë‚®ì€ í† í° ì œí•œ
        maxRequestLimit: 2   // 2íšŒ ìš”ì²­ë§Œ í—ˆìš©
    });

    console.log(`ì»¤ìŠ¤í…€ í† í° ì œí•œ: ${robota2.getMaxTokenLimit()}`);
    console.log(`ì»¤ìŠ¤í…€ ìš”ì²­ ì œí•œ: ${robota2.getMaxRequestLimit()}`);

    // 3. ìš”ì²­ ì‹¤í–‰ ë° ëª¨ë‹ˆí„°ë§
    console.log('\n=== ìš”ì²­ ì‹¤í–‰ ë° ëª¨ë‹ˆí„°ë§ ===');

    try {
        // ì²« ë²ˆì§¸ ìš”ì²­
        const response1 = await robota2.execute('ì•ˆë…•í•˜ì„¸ìš”');
        console.log(`âœ… ìš”ì²­ 1 ì„±ê³µ: ${response1.substring(0, 50)}...`);

        // í˜„ìž¬ ìƒíƒœ í™•ì¸
        const info1 = robota2.getLimitInfo();
        console.log(`ìƒíƒœ: í† í° ${info1.currentTokensUsed}/${info1.maxTokens}, ìš”ì²­ ${info1.currentRequestCount}/${info1.maxRequests}`);

        // ë‘ ë²ˆì§¸ ìš”ì²­
        const response2 = await robota2.execute('ê°ì‚¬í•©ë‹ˆë‹¤');
        console.log(`âœ… ìš”ì²­ 2 ì„±ê³µ: ${response2.substring(0, 50)}...`);

        const info2 = robota2.getLimitInfo();
        console.log(`ìƒíƒœ: í† í° ${info2.currentTokensUsed}/${info2.maxTokens}, ìš”ì²­ ${info2.currentRequestCount}/${info2.maxRequests}`);

        // ì„¸ ë²ˆì§¸ ìš”ì²­ (ì œí•œ ì´ˆê³¼)
        const response3 = await robota2.execute('ë˜ ë‹¤ë¥¸ ì§ˆë¬¸');
        console.log(`ìš”ì²­ 3: ${response3}`);

    } catch (error) {
        console.log(`âŒ ì œí•œ ì´ˆê³¼: ${(error as Error).message}`);
    }

    // 4. ë¬´ì œí•œ ì„¤ì •
    console.log('\n=== ë¬´ì œí•œ ì„¤ì • ===');
    const unlimitedRobota = new Robota({
        aiProviders: { 'openai': openaiProvider },
        currentProvider: 'openai',
        currentModel: 'gpt-3.5-turbo',
        maxTokenLimit: 0,    // ë¬´ì œí•œ
        maxRequestLimit: 0   // ë¬´ì œí•œ
    });

    const unlimitedInfo = unlimitedRobota.getLimitInfo();
    console.log(`ë¬´ì œí•œ ëª¨ë“œ: í† í°=${unlimitedInfo.isTokensUnlimited}, ìš”ì²­=${unlimitedInfo.isRequestsUnlimited}`);

    // 5. ì• ë„ë¦¬í‹±ìŠ¤ í™•ì¸
    console.log('\n=== ì• ë„ë¦¬í‹±ìŠ¤ ===');
    const analytics = robota2.getAnalytics();
    console.log(`ì´ ìš”ì²­: ${analytics.requestCount}`);
    console.log(`ì´ í† í°: ${analytics.totalTokensUsed}`);
    console.log(`í‰ê·  í† í°/ìš”ì²­: ${analytics.averageTokensPerRequest.toFixed(1)}`);

    // 6. ë™ì  ì œí•œ ë³€ê²½
    console.log('\n=== ë™ì  ì œí•œ ë³€ê²½ ===');
    robota2.setMaxTokenLimit(1000);
    robota2.setMaxRequestLimit(10);
    console.log(`ë³€ê²½ëœ í† í° ì œí•œ: ${robota2.getMaxTokenLimit()}`);
    console.log(`ë³€ê²½ëœ ìš”ì²­ ì œí•œ: ${robota2.getMaxRequestLimit()}`);

    console.log('\nâœ… ì˜ˆì œ ì™„ë£Œ!');
}

main().catch(error => {
    console.error('ì˜¤ë¥˜:', error);
}); 