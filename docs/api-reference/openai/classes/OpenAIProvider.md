<!-- 
 ⚠️  AUTO-GENERATED FILE - DO NOT EDIT MANUALLY
 This file is automatically generated by scripts/docs-generator.js
 To make changes, edit the source TypeScript files or update the generator script
-->

[openai](../../) / [Exports](../modules) / OpenAIProvider

# Class: OpenAIProvider

OpenAI AI provider implementation for Robota

Provides integration with OpenAI's GPT models and other services.
Extends BaseAIProvider for common functionality and tool calling support.

**`See`**

@examples/03-integrations | Provider Integration Examples

## Hierarchy

- `BaseAIProvider`

  ↳ **`OpenAIProvider`**

## Table of contents

### Constructors

- [constructor](OpenAIProvider#constructor)

### Properties

- [name](OpenAIProvider#name)
- [models](OpenAIProvider#models)
- [type](OpenAIProvider#type)
- [instance](OpenAIProvider#instance)
- [options](OpenAIProvider#options)

### Methods

- [initialize](OpenAIProvider#initialize)
- [dispose](OpenAIProvider#dispose)
- [isInitialized](OpenAIProvider#isinitialized)
- [execute](OpenAIProvider#execute)
- [executeStream](OpenAIProvider#executestream)
- [generateStreamingResponse](OpenAIProvider#generatestreamingresponse)
- [supportsModel](OpenAIProvider#supportsmodel)
- [formatFunctions](OpenAIProvider#formatfunctions)
- [generateResponse](OpenAIProvider#generateresponse)
- [chat](OpenAIProvider#chat)
- [parseResponse](OpenAIProvider#parseresponse)
- [parseStreamingChunk](OpenAIProvider#parsestreamingchunk)
- [chatStream](OpenAIProvider#chatstream)
- [close](OpenAIProvider#close)

## Constructors

### constructor

• **new OpenAIProvider**(`options`): [`OpenAIProvider`](OpenAIProvider)

Create a new OpenAI provider instance

#### Parameters

| Name | Type | Description |
| :------ | :------ | :------ |
| `options` | [`OpenAIProviderOptions`](../interfaces/OpenAIProviderOptions) | Configuration options for the OpenAI provider |

#### Returns

[`OpenAIProvider`](OpenAIProvider)

**`Throws`**

When client is not provided in options

#### Overrides

BaseAIProvider.constructor

#### Defined in

[openai/src/provider.ts:84](https://github.com/woojubb/robota/blob/0282eb7aeff4db97bfbe6e7aa549630531948e10/packages/openai/src/provider.ts#L84)

## Properties

### name

• `Readonly` **name**: `string` = `'openai'`

Provider identifier name

#### Overrides

BaseAIProvider.name

#### Defined in

[openai/src/provider.ts:31](https://github.com/woojubb/robota/blob/0282eb7aeff4db97bfbe6e7aa549630531948e10/packages/openai/src/provider.ts#L31)

___

### models

• `Readonly` **models**: `string`[]

Available models

#### Overrides

BaseAIProvider.models

#### Defined in

[openai/src/provider.ts:37](https://github.com/woojubb/robota/blob/0282eb7aeff4db97bfbe6e7aa549630531948e10/packages/openai/src/provider.ts#L37)

___

### type

• `Readonly` **type**: `string` = `'openai'`

Client type identifier

#### Defined in

[openai/src/provider.ts:56](https://github.com/woojubb/robota/blob/0282eb7aeff4db97bfbe6e7aa549630531948e10/packages/openai/src/provider.ts#L56)

___

### instance

• `Readonly` **instance**: `OpenAI`

OpenAI client instance (alias for backwards compatibility)

**`Deprecated`**

Use the private client property instead

#### Defined in

[openai/src/provider.ts:63](https://github.com/woojubb/robota/blob/0282eb7aeff4db97bfbe6e7aa549630531948e10/packages/openai/src/provider.ts#L63)

___

### options

• `Readonly` **options**: [`OpenAIProviderOptions`](../interfaces/OpenAIProviderOptions)

Provider configuration options

#### Defined in

[openai/src/provider.ts:69](https://github.com/woojubb/robota/blob/0282eb7aeff4db97bfbe6e7aa549630531948e10/packages/openai/src/provider.ts#L69)

## Methods

### initialize

▸ **initialize**(): `Promise`\<`void`\>

#### Returns

`Promise`\<`void`\>

#### Inherited from

BaseAIProvider.initialize

#### Defined in

agents/dist/index.d.ts:848

___

### dispose

▸ **dispose**(): `Promise`\<`void`\>

#### Returns

`Promise`\<`void`\>

#### Inherited from

BaseAIProvider.dispose

#### Defined in

agents/dist/index.d.ts:849

___

### isInitialized

▸ **isInitialized**(): `boolean`

#### Returns

`boolean`

#### Inherited from

BaseAIProvider.isInitialized

#### Defined in

agents/dist/index.d.ts:850

___

### execute

▸ **execute**(`messages`, `config`): `Promise`\<`ProviderExecutionResult`\>

High-level execute method that handles the entire conversation process

This method encapsulates all provider-specific logic including:
- Message format conversion
- Tool configuration
- Request preparation
- Response processing

ExecutionService delegates the entire AI interaction to this method.

#### Parameters

| Name | Type | Description |
| :------ | :------ | :------ |
| `messages` | `UniversalMessage`[] | Array of UniversalMessage from conversation history |
| `config` | `ProviderExecutionConfig` | Execution configuration |

#### Returns

`Promise`\<`ProviderExecutionResult`\>

Provider execution result

#### Inherited from

BaseAIProvider.execute

#### Defined in

agents/dist/index.d.ts:900

___

### executeStream

▸ **executeStream**(`messages`, `config`): `AsyncGenerator`\<`ProviderExecutionResult`, `void`, `unknown`\>

High-level streaming execute method

#### Parameters

| Name | Type | Description |
| :------ | :------ | :------ |
| `messages` | `UniversalMessage`[] | Array of UniversalMessage from conversation history |
| `config` | `ProviderExecutionConfig` | Execution configuration |

#### Returns

`AsyncGenerator`\<`ProviderExecutionResult`, `void`, `unknown`\>

Async generator of streaming results

#### Inherited from

BaseAIProvider.executeStream

#### Defined in

agents/dist/index.d.ts:908

___

### generateStreamingResponse

▸ **generateStreamingResponse**(`request`): `AsyncGenerator`\<`any`, `void`, `unknown`\>

Generate streaming response using raw request payload (default implementation uses chatStream)

#### Parameters

| Name | Type |
| :------ | :------ |
| `request` | `any` |

#### Returns

`AsyncGenerator`\<`any`, `void`, `unknown`\>

#### Inherited from

BaseAIProvider.generateStreamingResponse

#### Defined in

agents/dist/index.d.ts:916

___

### supportsModel

▸ **supportsModel**(`model`): `boolean`

#### Parameters

| Name | Type |
| :------ | :------ |
| `model` | `string` |

#### Returns

`boolean`

#### Inherited from

BaseAIProvider.supportsModel

#### Defined in

agents/dist/index.d.ts:917

___

### formatFunctions

▸ **formatFunctions**(`tools`): `ChatCompletionTool`[]

Convert tool definitions to OpenAI tool format

Transforms universal tool definitions into OpenAI's specific tool format
required by the Chat Completions API.

#### Parameters

| Name | Type | Description |
| :------ | :------ | :------ |
| `tools` | `ToolSchema`[] | Array of universal tool definitions |

#### Returns

`ChatCompletionTool`[]

Array of OpenAI-formatted tools

#### Defined in

[openai/src/provider.ts:122](https://github.com/woojubb/robota/blob/0282eb7aeff4db97bfbe6e7aa549630531948e10/packages/openai/src/provider.ts#L122)

___

### generateResponse

▸ **generateResponse**(`request`): `Promise`\<`any`\>

Generate response using raw request payload (for agents package compatibility)

This method is required by the agents package's ConversationService.
It adapts the raw request payload to the OpenAI API format.

#### Parameters

| Name | Type | Description |
| :------ | :------ | :------ |
| `request` | `any` | Raw request payload from ConversationService |

#### Returns

`Promise`\<`any`\>

Promise resolving to OpenAI API response

#### Overrides

BaseAIProvider.generateResponse

#### Defined in

[openai/src/provider.ts:171](https://github.com/woojubb/robota/blob/0282eb7aeff4db97bfbe6e7aa549630531948e10/packages/openai/src/provider.ts#L171)

___

### chat

▸ **chat**(`model`, `context`, `options?`): `Promise`\<`ModelResponse`\>

Send a chat request to OpenAI and receive a complete response

Processes the provided context and sends it to OpenAI's Chat Completions API.
Handles message format conversion, error handling, and response parsing.

#### Parameters

| Name | Type | Description |
| :------ | :------ | :------ |
| `model` | `string` | Model name to use (e.g., 'gpt-4', 'gpt-3.5-turbo') |
| `context` | `Context` | Context object containing messages and system prompt |
| `options?` | `any` | Optional generation parameters and tools |

#### Returns

`Promise`\<`ModelResponse`\>

Promise resolving to the model's response

**`Throws`**

When context is invalid

**`Throws`**

When messages array is invalid

**`Throws`**

When message format conversion fails

**`Throws`**

When OpenAI API call fails

#### Overrides

BaseAIProvider.chat

#### Defined in

[openai/src/provider.ts:228](https://github.com/woojubb/robota/blob/0282eb7aeff4db97bfbe6e7aa549630531948e10/packages/openai/src/provider.ts#L228)

___

### parseResponse

▸ **parseResponse**(`response`): `ModelResponse`

Convert OpenAI API response to universal ModelResponse format

Transforms the OpenAI-specific response format into the standard format
used across all providers in Robota.

#### Parameters

| Name | Type | Description |
| :------ | :------ | :------ |
| `response` | `ChatCompletion` | Raw response from OpenAI Chat Completions API |

#### Returns

`ModelResponse`

Parsed model response in universal format

#### Defined in

[openai/src/provider.ts:290](https://github.com/woojubb/robota/blob/0282eb7aeff4db97bfbe6e7aa549630531948e10/packages/openai/src/provider.ts#L290)

___

### parseStreamingChunk

▸ **parseStreamingChunk**(`chunk`): `StreamingResponseChunk`

Convert OpenAI streaming response chunk to universal format

Transforms individual chunks from OpenAI's streaming response into the
standard StreamingResponseChunk format used across all providers.

#### Parameters

| Name | Type | Description |
| :------ | :------ | :------ |
| `chunk` | `ChatCompletionChunk` | Raw chunk from OpenAI streaming API |

#### Returns

`StreamingResponseChunk`

Parsed streaming response chunk

#### Defined in

[openai/src/provider.ts:333](https://github.com/woojubb/robota/blob/0282eb7aeff4db97bfbe6e7aa549630531948e10/packages/openai/src/provider.ts#L333)

___

### chatStream

▸ **chatStream**(`model`, `context`, `options?`): `AsyncGenerator`\<`StreamingResponseChunk`, `void`, `unknown`\>

Send a streaming chat request to OpenAI and receive response chunks

Similar to chat() but returns an async iterator that yields response chunks
as they arrive from OpenAI's streaming API. Useful for real-time display
of responses or handling large responses incrementally.

#### Parameters

| Name | Type | Description |
| :------ | :------ | :------ |
| `model` | `string` | Model name to use |
| `context` | `Context` | Context object containing messages and system prompt |
| `options?` | `any` | Optional generation parameters and tools |

#### Returns

`AsyncGenerator`\<`StreamingResponseChunk`, `void`, `unknown`\>

Async generator yielding response chunks

**`Throws`**

When context is invalid

**`Throws`**

When messages array is invalid

**`Throws`**

When message format conversion fails

**`Throws`**

When OpenAI streaming API call fails

**`See`**

@examples/01-basic | Basic Usage Examples

#### Overrides

BaseAIProvider.chatStream

#### Defined in

[openai/src/provider.ts:377](https://github.com/woojubb/robota/blob/0282eb7aeff4db97bfbe6e7aa549630531948e10/packages/openai/src/provider.ts#L377)

___

### close

▸ **close**(): `Promise`\<`void`\>

Release resources and close connections

Performs cleanup operations when the provider is no longer needed.
OpenAI client doesn't require explicit cleanup, so this is a no-op.

#### Returns

`Promise`\<`void`\>

Promise that resolves when cleanup is complete

#### Overrides

BaseAIProvider.close

#### Defined in

[openai/src/provider.ts:454](https://github.com/woojubb/robota/blob/0282eb7aeff4db97bfbe6e7aa549630531948e10/packages/openai/src/provider.ts#L454)
