<!-- 
 ⚠️  AUTO-GENERATED FILE - DO NOT EDIT MANUALLY
 This file is automatically generated by scripts/docs-generator.js
 To make changes, edit the source TypeScript files or update the generator script
-->

[openai](../../) / [Exports](../modules) / OpenAIProvider

# Class: OpenAIProvider

OpenAI provider implementation for Robota

Provides integration with OpenAI's GPT models and other services.
Implements the universal AIProvider interface for consistent usage across providers.

**`See`**

../../../apps/examples/03-integrations | Provider Integration Examples

## Implements

- `AIProvider`

## Table of contents

### Constructors

- [constructor](OpenAIProvider#constructor)

### Properties

- [name](OpenAIProvider#name)
- [type](OpenAIProvider#type)
- [instance](OpenAIProvider#instance)
- [options](OpenAIProvider#options)

### Methods

- [formatFunctions](OpenAIProvider#formatfunctions)
- [chat](OpenAIProvider#chat)
- [parseResponse](OpenAIProvider#parseresponse)
- [parseStreamingChunk](OpenAIProvider#parsestreamingchunk)
- [chatStream](OpenAIProvider#chatstream)
- [close](OpenAIProvider#close)

## Constructors

### constructor

• **new OpenAIProvider**(`options`): [`OpenAIProvider`](OpenAIProvider)

Create a new OpenAI provider instance

#### Parameters

| Name | Type | Description |
| :------ | :------ | :------ |
| `options` | [`OpenAIProviderOptions`](../interfaces/OpenAIProviderOptions) | Configuration options for the OpenAI provider |

#### Returns

[`OpenAIProvider`](OpenAIProvider)

**`Throws`**

When client is not provided in options

#### Defined in

[openai/src/provider.ts:62](https://github.com/woojubb/robota/blob/b8c05a1e0e0191a7c7da275868f2aa9a78af55c1/packages/openai/src/provider.ts#L62)

## Properties

### name

• `Readonly` **name**: `string` = `'openai'`

Provider identifier name

#### Implementation of

AIProvider.name

#### Defined in

[openai/src/provider.ts:28](https://github.com/woojubb/robota/blob/b8c05a1e0e0191a7c7da275868f2aa9a78af55c1/packages/openai/src/provider.ts#L28)

___

### type

• `Readonly` **type**: `string` = `'openai'`

Client type identifier

#### Defined in

[openai/src/provider.ts:40](https://github.com/woojubb/robota/blob/b8c05a1e0e0191a7c7da275868f2aa9a78af55c1/packages/openai/src/provider.ts#L40)

___

### instance

• `Readonly` **instance**: `OpenAI`

OpenAI client instance (alias for backwards compatibility)

**`Deprecated`**

Use the private client property instead

#### Defined in

[openai/src/provider.ts:47](https://github.com/woojubb/robota/blob/b8c05a1e0e0191a7c7da275868f2aa9a78af55c1/packages/openai/src/provider.ts#L47)

___

### options

• `Readonly` **options**: [`OpenAIProviderOptions`](../interfaces/OpenAIProviderOptions)

Provider configuration options

#### Defined in

[openai/src/provider.ts:53](https://github.com/woojubb/robota/blob/b8c05a1e0e0191a7c7da275868f2aa9a78af55c1/packages/openai/src/provider.ts#L53)

## Methods

### formatFunctions

▸ **formatFunctions**(`functions`): `ChatCompletionTool`[]

Convert function definitions to OpenAI tool format

Transforms universal function definitions into OpenAI's specific tool format
required by the Chat Completions API.

#### Parameters

| Name | Type | Description |
| :------ | :------ | :------ |
| `functions` | `FunctionDefinition`[] | Array of universal function definitions |

#### Returns

`ChatCompletionTool`[]

Array of OpenAI-formatted tools

#### Defined in

[openai/src/provider.ts:87](https://github.com/woojubb/robota/blob/b8c05a1e0e0191a7c7da275868f2aa9a78af55c1/packages/openai/src/provider.ts#L87)

___

### chat

▸ **chat**(`model`, `context`, `options?`): `Promise`\<`ModelResponse`\>

Send a chat request to OpenAI and receive a complete response

Processes the provided context and sends it to OpenAI's Chat Completions API.
Handles message format conversion, error handling, and response parsing.

#### Parameters

| Name | Type | Description |
| :------ | :------ | :------ |
| `model` | `string` | Model name to use (e.g., 'gpt-4', 'gpt-3.5-turbo') |
| `context` | `Context` | Context object containing messages and system prompt |
| `options?` | `any` | Optional generation parameters and tools |

#### Returns

`Promise`\<`ModelResponse`\>

Promise resolving to the model's response

**`Throws`**

When context is invalid

**`Throws`**

When messages array is invalid

**`Throws`**

When message format conversion fails

**`Throws`**

When OpenAI API call fails

#### Implementation of

AIProvider.chat

#### Defined in

[openai/src/provider.ts:114](https://github.com/woojubb/robota/blob/b8c05a1e0e0191a7c7da275868f2aa9a78af55c1/packages/openai/src/provider.ts#L114)

___

### parseResponse

▸ **parseResponse**(`response`): `ModelResponse`

Convert OpenAI API response to universal ModelResponse format

Transforms the OpenAI-specific response format into the standard format
used across all providers in Robota.

#### Parameters

| Name | Type | Description |
| :------ | :------ | :------ |
| `response` | `ChatCompletion` | Raw response from OpenAI Chat Completions API |

#### Returns

`ModelResponse`

Parsed model response in universal format

#### Defined in

[openai/src/provider.ts:188](https://github.com/woojubb/robota/blob/b8c05a1e0e0191a7c7da275868f2aa9a78af55c1/packages/openai/src/provider.ts#L188)

___

### parseStreamingChunk

▸ **parseStreamingChunk**(`chunk`): `StreamingResponseChunk`

Convert OpenAI streaming response chunk to universal format

Transforms individual chunks from OpenAI's streaming response into the
standard StreamingResponseChunk format used across all providers.

#### Parameters

| Name | Type | Description |
| :------ | :------ | :------ |
| `chunk` | `ChatCompletionChunk` | Raw chunk from OpenAI streaming API |

#### Returns

`StreamingResponseChunk`

Parsed streaming response chunk

#### Defined in

[openai/src/provider.ts:238](https://github.com/woojubb/robota/blob/b8c05a1e0e0191a7c7da275868f2aa9a78af55c1/packages/openai/src/provider.ts#L238)

___

### chatStream

▸ **chatStream**(`model`, `context`, `options?`): `AsyncGenerator`\<`StreamingResponseChunk`, `void`, `unknown`\>

Send a streaming chat request to OpenAI and receive response chunks

Similar to chat() but returns an async iterator that yields response chunks
as they arrive from OpenAI's streaming API. Useful for real-time display
of responses or handling large responses incrementally.

#### Parameters

| Name | Type | Description |
| :------ | :------ | :------ |
| `model` | `string` | Model name to use |
| `context` | `Context` | Context object containing messages and system prompt |
| `options?` | `any` | Optional generation parameters and tools |

#### Returns

`AsyncGenerator`\<`StreamingResponseChunk`, `void`, `unknown`\>

Async generator yielding response chunks

**`Throws`**

When context is invalid

**`Throws`**

When messages array is invalid

**`Throws`**

When message format conversion fails

**`Throws`**

When OpenAI streaming API call fails

**`See`**

../../../apps/examples/01-basic | Basic Usage Examples

#### Implementation of

AIProvider.chatStream

#### Defined in

[openai/src/provider.ts:287](https://github.com/woojubb/robota/blob/b8c05a1e0e0191a7c7da275868f2aa9a78af55c1/packages/openai/src/provider.ts#L287)

___

### close

▸ **close**(): `Promise`\<`void`\>

Release resources and close connections

Performs cleanup operations when the provider is no longer needed.
OpenAI client doesn't require explicit cleanup, so this is a no-op.

#### Returns

`Promise`\<`void`\>

Promise that resolves when cleanup is complete

#### Implementation of

AIProvider.close

#### Defined in

[openai/src/provider.ts:366](https://github.com/woojubb/robota/blob/b8c05a1e0e0191a7c7da275868f2aa9a78af55c1/packages/openai/src/provider.ts#L366)
